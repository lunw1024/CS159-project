{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17548,"status":"ok","timestamp":1716709907518,"user":{"displayName":"Enoch Luk","userId":"13925640396040210512"},"user_tz":420},"id":"1OSAogmRuqgp","outputId":"5efdb1dd-64d2-4ffa-8cea-bd369b0aae9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: together in /usr/local/lib/python3.10/dist-packages (1.2.0)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.9.3 in /usr/local/lib/python3.10/dist-packages (from together) (3.9.5)\n","Requirement already satisfied: click<9.0.0,>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from together) (8.1.7)\n","Requirement already satisfied: eval-type-backport<0.3.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from together) (0.2.0)\n","Requirement already satisfied: filelock<4.0.0,>=3.13.1 in /usr/local/lib/python3.10/dist-packages (from together) (3.14.0)\n","Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from together) (1.25.2)\n","Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /usr/local/lib/python3.10/dist-packages (from together) (10.3.0)\n","Requirement already satisfied: pyarrow>=10.0.1 in /usr/local/lib/python3.10/dist-packages (from together) (14.0.2)\n","Requirement already satisfied: pydantic<3.0.0,>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from together) (2.7.1)\n","Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from together) (2.31.0)\n","Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from together) (0.9.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.66.2 in /usr/local/lib/python3.10/dist-packages (from together) (4.66.4)\n","Requirement already satisfied: typer<0.13,>=0.9 in /usr/local/lib/python3.10/dist-packages (from together) (0.9.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.3->together) (4.0.3)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.3->together) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.3->together) (2.18.2)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.6.3->together) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->together) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->together) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->together) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->together) (2024.2.2)\n"]}],"source":["!pip install together\n","from together import Together"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":26932,"status":"ok","timestamp":1716709934435,"user":{"displayName":"Enoch Luk","userId":"13925640396040210512"},"user_tz":420},"id":"x-vB7FCSkHdy","outputId":"af81e4ad-f28c-4902-c9db-5e98a5e1bf5d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["import os\n","import sys\n","sys.path.append('/content/drive/MyDrive/CS 159')\n","\n","from newest_utils2 import extract_answer, evaluate_file, evaluate_across_files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oN94QszBcCve"},"outputs":[],"source":["import re\n","import glob\n","import requests\n","import pandas as pd\n","pd.set_option(\"display.max_columns\", None)\n","import numpy as np\n","from tqdm.notebook import tqdm\n","\n","\n","data_path = \"/content/drive/MyDrive/CS 159/data/raw/SpatialEvalLLM/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0LoL9t0HaEQV"},"outputs":[],"source":["class BaseModelClient:\n","    def __init__(self, api_key, model_path):\n","        self.client = Together(api_key=api_key)\n","        self.model_path = model_path\n","\n","    def predict(self, prompt):\n","        response = self.client.chat.completions.create(\n","            model=self.model_path,\n","            messages=[{\"role\": \"user\", \"content\": prompt}]\n","        )\n","        return response.choices[0].message.content\n","\n","class Llama8b(BaseModelClient):\n","    def __init__(self, api_key):\n","        super().__init__(api_key, \"meta-llama/Llama-3-8b-chat-hf\")\n","\n","class Llama70b(BaseModelClient):\n","    def __init__(self, api_key):\n","        super().__init__(api_key, \"meta-llama/Llama-3-70b-chat-hf\")\n","\n","api_key = \"--redacted--\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":232284,"status":"ok","timestamp":1716710700491,"user":{"displayName":"Enoch Luk","userId":"13925640396040210512"},"user_tz":420},"id":"M-eZjdYDwNzn","outputId":"00392cda-954e-4435-a2f8-8738efe559a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Intermediate results saved to /content/drive/MyDrive/CS 159/Llama70b/type-ring_size-12_steps-8_seed-12_n-100_evaluation_code_code_use.csv\n","                                             Question      Expected Answer  \\\n","0   You have been given a circular path consisting...                stupa   \n","1   You have been given a circular path consisting...  typewriter keyboard   \n","2   You have been given a circular path consisting...    threshing machine   \n","3   You have been given a circular path consisting...                 tent   \n","4   You have been given a circular path consisting...        ballpoint pen   \n","..                                                ...                  ...   \n","95  You have been given a circular path consisting...                 jeep   \n","96  You have been given a circular path consisting...              bow tie   \n","97  You have been given a circular path consisting...           petri dish   \n","98  You have been given a circular path consisting...              meerkat   \n","99  You have been given a circular path consisting...        affenpinscher   \n","\n","                                       Model Response  \\\n","0   ###code###\\npath = ['crash helmet', 'trimaran'...   \n","1   ###code###\\npath = ['hair spray', 'typewriter ...   \n","2   ###code###\\npath = ['gorilla', 'chain-link fen...   \n","3   ###code###\\npath = [\"mousetrap\", \"home theater...   \n","4   ###code###\\npath = ['ballpoint pen', 'gossamer...   \n","..                                                ...   \n","95  ###code###\\npath = ['jeep', 'lighter', 'beaver...   \n","96  ###code###\\nelements = ['mosquito net', 'radio...   \n","97  ###code###\\npath = ['mixing bowl', 'holster', ...   \n","98  ###code###\\npath = ['tights', 'nipple', 'fig',...   \n","99  ###code###\\npath = ['affenpinscher', 'dromedar...   \n","\n","              Extracted Answer  Is Correct  \n","0                        stupa        True  \n","1          typewriter keyboard        True  \n","2            threshing machine        True  \n","3                         tent        True  \n","4                ballpoint pen        True  \n","..                         ...         ...  \n","95  name 'path' is not defined       False  \n","96                     bow tie        True  \n","97                  petri dish        True  \n","98                     meerkat        True  \n","99                   dromedary       False  \n","\n","[100 rows x 5 columns]\n","232.0215048789978\n"]}],"source":["# import time\n","# use_code = True\n","# start = time.time()\n","# model =  Llama70b(api_key)\n","# leading_prompt = \"\"\n","# if use_code:\n","#     trailing_prompt = \"Output a python script that will calculate the solution in between ###code### delimiters ###endcode###. No print statements. At the end of evaluation, set the variable 'result' to the answer, in lower case with no articles 'the' or 'a'.\"\n","# else:\n","#     trailing_prompt = \"Visualize the state after each step. Place the answer, in lower case, with asteriks before and after like *this*. No asteriks anywhere else in the response.\"\n","\n","# file_path = \"/content/drive/MyDrive/CS 159/data/raw/SpatialEvalLLM/map_global/type-ring_size-12_steps-8_seed-12_n-100.jsonl\"\n","\n","\n","# results_df = evaluate_file(file_path, model, leading_prompt, trailing_prompt, use_code, \"code_use\", )\n","\n","# print(results_df)\n","# print(time.time() - start)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fEwYUBVly1bN"},"outputs":[],"source":["import glob\n","import pandas as pd\n","import re\n","import concurrent.futures\n","from tqdm.notebook import tqdm\n","from pathlib import Path\n","\n","\n","def process_model_evaluation(model_name, model_instance, data_path, leading_prompt, trailing_prompt, suffix):\n","    print(f\"Starting evaluation for {model_name} with trailing prompt: {trailing_prompt[:30]}...\")\n","\n","    scores_df = evaluate_across_files(\n","        model=model_instance,\n","        leading_prompt=leading_prompt,\n","        trailing_prompt=trailing_prompt,\n","        data_path=data_path,\n","        suffix=suffix,\n","        use_code=False,\n","        save=True,\n","    )\n","\n","    save_path = f\"/content/drive/MyDrive/CS 159/{model_name}_evaluation_{suffix}.csv\"\n","\n","    scores_df.to_csv(save_path, index=False)\n","    print(f\"Results saved to {save_path}\")\n","\n","models = {\n","    \"Llama70b\": Llama70b(api_key),\n","    \"Llama8b\": Llama8b(api_key)\n","}\n","\n","\n","leading_prompt = \"\"\n","suffixes = [\"indexing\"] #\"baseline\", \"zero_shot_cot\", \"zero_shot_vot\"]\n","\n","trailing_prompts = [\n","    \"\\n\\nAssign a numerical index to each element on the path and use this system to track your position.\\n\\nPlace the answer, in lower case, with asterisks before and after like *this*. No asterisks anywhere else in the response.\",\n","    # \"\\n\\nExplain your reasoning step-by-step.\\n\\nPlace the answer, in lower case, with asterisks before and after like *this*. No asterisks anywhere else in the response.\",\n","    # \"\\n\\nVisualize the state of the map after each step.\\n\\nVisualize the state after each step.\\n\\nPlace the answer, in lower case, with asterisks before and after like *this*. No asterisks anywhere else in the response.\"\n","]\n","\n","with concurrent.futures.ThreadPoolExecutor(max_workers=len(models) * len(suffixes)) as executor:\n","    futures = [\n","        executor.submit(process_model_evaluation, model_name, model_instance, data_path, leading_prompt, trailing_prompt, suffix)\n","        for model_name, model_instance in models.items()\n","        for trailing_prompt, suffix in zip(trailing_prompts, suffixes)\n","    ]\n","    concurrent.futures.wait(futures)\n","\n","print(\"All evaluations are complete.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eu3L2MIuMFdD"},"outputs":[],"source":["# internal representations\n","models = {\n","    \"Llama70b\": Llama70b(api_key),\n","    \"Llama8b\": Llama8b(api_key)\n","}\n","model_instance = Llama8b(api_key)  # assuming this is your model instance\n","leading_prompt = \"\"\n","\n","prompts = {\n","    'base': '\\nPresent reasoning step-by-step. Place the answer, in lower case, with asterisks before and after like *this*. No asterisks anywhere else in the response.',\n","    'grid': '''\\nPresent reasoning step-by-step.\\nAt each step, represent the current state (including your current location) as\n","+-----------+-----------+---------+\n","| Object 1 | Object 2 | Object 3 |\n","+-----------+-----------+-----------+\n","| Object 4 | Object 5* | Object 6 |\n","+----------+-----------+-----------+\n","| Object 7 | Object 8 | Object 9 |\n","+----------+-----------+----------+\n","where the asterisk * corresponds to the current location.\n","Place the answer, in lower case, with asterisks before and after like *this*. No asterisks anywhere else in the response.''',\n","    'csv': '''\\nPresent reasoning step-by-step.\\nAt each step, represent the current state (including your current location) as\n","Object 1, Object 2, Object 3\n","Object 4, Object 5*, Object 6\n","Object 7, Object 8, Object 9\n","where the asterisk * corresponds to the current location.\n","Place the answer, in lower case, with asterisks before and after like *this*. No asterisks anywhere else in the response.''',\n","    'coord': '''\\nPresent reasoning step-by-step.\\nAt each step, represent the current state (including your current location) as\n","(1, 1): Object 1, (1, 2): Object 2, (1, 3): Object 3, (2, 1): Object 4, (2, 2): Object 5*, (2, 3): Object 6, (3, 1): Object 7, (3, 2): Object 8, (3, 3): Object 9\n","where the asterisk * corresponds to the current location.\n","Place the answer, in lower case, with asterisks before and after like *this*. No asterisks anywhere else in the response.''',\n","    'colbycol': '''\\nPresent reasoning step-by-step.\\nAt each step, represent the current state (including your current location) as\n","Object 1, Object 4, Object 7\n","Object 2, Object 5*, Object 8\n","Object 3, Object 6, Object 9\n","where the asterisk * corresponds to the current location.\n","Place the answer, in lower case, with asterisks before and after like *this*. No asterisks anywhere else in the response.''',\n","}\n","# trailing_prompt = \"\\n\\nVisualize the state of the map after each step.\\n\\nVisualize the state after each step.\\n\\nPlace the answer, in lower case, with asterisks before and after like *this*. No asterisks anywhere else in the response.\"\n","\n","for model_name, model_instance in models.items():\n","    for label, prompt in prompts.items():\n","        save_path = f'/content/drive/MyDrive/CS 159/internal_representations/type-square_size-3_steps-4_seed-3_n-100_{model_name}_' + label + '.csv'\n","        print('Running', save_path)\n","        # Run the evaluation for the specific configuration\n","        # process_model_evaluation(model_name, model_instance, data_path, leading_prompt, trailing_prompt, suffix)\n","        df = evaluate_file(\n","            data_path + 'map_global/type-square_size-3_steps-4_seed-3_n-100.jsonl',\n","            model_instance,\n","            leading_prompt,\n","            prompt,\n","            use_code=False,\n","            suffix=label,\n","            pbar=tqdm(),\n","            debug=False\n","        )\n","        df.to_csv(save_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qH6Gro0exThF"},"outputs":[],"source":["# debug\n","import glob\n","result_files = glob.glob('/content/drive/MyDrive/CS 159/internal_representations/*.csv')\n","for file in result_files:\n","    df = pd.read_csv(file)\n","    print('=================================')\n","    print(file)\n","    print(df.at[0, 'Model Response'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3kDQaFKJCFsp"},"outputs":[],"source":["print(df.at[0, 'Model Response'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8uLAg2SAwhJT"},"outputs":[],"source":["# import pandas as pd\n","# import glob\n","\n","# # Load the results from the files\n","# result_files = glob.glob(\"/content/drive/MyDrive/CS 159/*.csv\")\n","\n","# # Initialize a dictionary to hold the aggregated results\n","# aggregated_results = {}\n","\n","# for file in result_files:\n","#     # Extract the model name and code mode from the filename\n","#     filename = file.split(\"/\")[-1]\n","#     model_name = filename.split(\"_evaluation_\")[0]\n","#     file_mode = filename.split(\"_evaluation_\")[1].split(\".csv\")[0]\n","\n","#     # Load the CSV file\n","#     df = pd.read_csv(file)\n","\n","#     if model_name not in aggregated_results:\n","#         aggregated_results[model_name] = {\"code\": {\"Correct\": 0, \"Total\": 0},\n","#                                           \"no_code\": {\"Correct\": 0, \"Total\": 0}}\n","\n","#     # Sum up the correct and total counts for each mode\n","#     aggregated_results[model_name][file_mode][\"Correct\"] += df[\"Correct\"].sum()\n","#     aggregated_results[model_name][file_mode][\"Total\"] += df[\"Total\"].sum()\n","\n","# # Create a DataFrame to display the results\n","# summary_data = []\n","# for model, results in aggregated_results.items():\n","#     for mode in [\"code\", \"no_code\"]:\n","#         accuracy = results[mode][\"Correct\"] / results[mode][\"Total\"] if results[mode][\"Total\"] != 0 else 0\n","#         summary_data.append({\"Model\": model, \"Mode\": mode, \"Accuracy\": accuracy})\n","\n","# summary_df = pd.DataFrame(summary_data)\n","\n","# # Display the aggregated results\n","# display(summary_df)\n"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1DofOtLlAUeDh4S28fk_KPHtHMlYuLRPD","timestamp":1714772464556}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
